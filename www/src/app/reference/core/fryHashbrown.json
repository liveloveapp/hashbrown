{
  "name": "fryHashbrown",
  "canonicalReference": "@hashbrownai/core!fryHashbrown:function",
  "kind": "Function",
  "fileUrlPath": "../packages/core/src/hashbrown.ts",
  "isDeprecated": false,
  "members": [
    {
      "kind": "Function",
      "canonicalReference": "@hashbrownai/core!fryHashbrown:function(1)",
      "docComment": "/**\n * Initialize a Hashbrown chat instance with the given configuration.\n *\n * @template Output - The type of messages expected from the LLM.  @template Tools - The set of tools to register with the chat instance.\n *\n * @param init - Initialization options.\n *\n * @param  - {string} [init.debugName] - Optional debug name for devtools tracing.\n *\n * @param  - {string} init.apiUrl - Base URL of the Hashbrown API endpoint.\n *\n * @param  - {string} init.model - The LLM model identifier to use.\n *\n * @param  - {string} init.prompt - System prompt or initial context for the chat.\n *\n * @param  - {number} [init.temperature] - Sampling temperature for LLM responses.\n *\n * @param  - {number} [init.maxTokens] - Maximum tokens to generate per response.\n *\n * @param  - {Chat.Message<Output, Tools>[]} [init.messages] - Initial message history.\n *\n * @param  - {Tools[]} [init.tools] - Array of tools to enable in the instance.\n *\n * @param  - {s.HashbrownType} [init.responseSchema] - JSON schema for validating structured output.\n *\n * @param  - {Chat.Middleware[]} [init.middleware] - Middleware functions to run on messages.\n *\n * @param  - {boolean} [init.emulateStructuredOutput] - Whether to emulate structured output behavior.\n *\n * @param  - {number} [init.debounce] - Debounce interval in milliseconds for sending messages.\n *\n * @returns {Hashbrown<Output, Tools>} A configured Hashbrown instance.\n *\n * @throws\n *\n * {Error} If a reserved tool name (\"output\") is used.\n */\n",
      "excerptTokens": [
        {
          "kind": "Content",
          "text": "export declare function fryHashbrown<Output extends "
        },
        { "kind": "Content", "text": "string | " },
        {
          "kind": "Reference",
          "text": "s.HashbrownType",
          "canonicalReference": "@hashbrownai/core!s.HashbrownType:interface"
        },
        { "kind": "Content", "text": ", Tools extends " },
        {
          "kind": "Reference",
          "text": "Chat.AnyTool",
          "canonicalReference": "@hashbrownai/core!~Chat_2.AnyTool:type"
        },
        { "kind": "Content", "text": ">(init: " },
        {
          "kind": "Content",
          "text": "{\n    debugName?: string;\n    apiUrl: string;\n    model: string;\n    prompt: string;\n    temperature?: number;\n    maxTokens?: number;\n    messages?: "
        },
        {
          "kind": "Reference",
          "text": "Chat.Message",
          "canonicalReference": "@hashbrownai/core!~Chat_2.Message_3:type"
        },
        {
          "kind": "Content",
          "text": "<Output, Tools>[];\n    tools?: Tools[];\n    responseSchema?: "
        },
        {
          "kind": "Reference",
          "text": "s.HashbrownType",
          "canonicalReference": "@hashbrownai/core!s.HashbrownType:interface"
        },
        { "kind": "Content", "text": ";\n    middleware?: " },
        {
          "kind": "Reference",
          "text": "Chat.Middleware",
          "canonicalReference": "@hashbrownai/core!~Chat_2.Middleware:type"
        },
        {
          "kind": "Content",
          "text": "[];\n    emulateStructuredOutput?: boolean;\n    debounce?: number;\n}"
        },
        { "kind": "Content", "text": "): " },
        {
          "kind": "Reference",
          "text": "Hashbrown",
          "canonicalReference": "@hashbrownai/core!Hashbrown:interface"
        },
        { "kind": "Content", "text": "<Output, Tools>" },
        { "kind": "Content", "text": ";" }
      ],
      "fileUrlPath": "../packages/core/src/hashbrown.ts",
      "returnTypeTokenRange": { "startIndex": 14, "endIndex": 16 },
      "releaseTag": "Public",
      "overloadIndex": 1,
      "parameters": [
        {
          "parameterName": "init",
          "parameterTypeTokenRange": { "startIndex": 6, "endIndex": 13 },
          "isOptional": false
        }
      ],
      "typeParameters": [
        {
          "typeParameterName": "Output",
          "constraintTokenRange": { "startIndex": 1, "endIndex": 3 },
          "defaultTypeTokenRange": { "startIndex": 0, "endIndex": 0 }
        },
        {
          "typeParameterName": "Tools",
          "constraintTokenRange": { "startIndex": 4, "endIndex": 5 },
          "defaultTypeTokenRange": { "startIndex": 0, "endIndex": 0 }
        }
      ],
      "name": "fryHashbrown",
      "docs": {
        "modifiers": {
          "isInternal": false,
          "isPublic": false,
          "isAlpha": false,
          "isBeta": false,
          "isOverride": false,
          "isExperimental": false
        },
        "summary": "Initialize a Hashbrown chat instance with the given configuration.\n\n@template Output - The type of messages expected from the LLM.  @template Tools - The set of tools to register with the chat instance.\n\n",
        "usageNotes": "",
        "remarks": "",
        "deprecated": "",
        "returns": " {Hashbrown<Output, Tools>} A configured Hashbrown instance.\n\n",
        "see": [],
        "params": [
          { "name": "init", "description": "Initialization options.\n\n" },
          {
            "name": "",
            "description": "  - {string} [init.debugName] - Optional debug name for devtools tracing.\n\n"
          },
          {
            "name": "",
            "description": "  - {string} init.apiUrl - Base URL of the Hashbrown API endpoint.\n\n"
          },
          {
            "name": "",
            "description": "  - {string} init.model - The LLM model identifier to use.\n\n"
          },
          {
            "name": "",
            "description": "  - {string} init.prompt - System prompt or initial context for the chat.\n\n"
          },
          {
            "name": "",
            "description": "  - {number} [init.temperature] - Sampling temperature for LLM responses.\n\n"
          },
          {
            "name": "",
            "description": "  - {number} [init.maxTokens] - Maximum tokens to generate per response.\n\n"
          },
          {
            "name": "",
            "description": "  - {Chat.Message<Output, Tools>[]} [init.messages] - Initial message history.\n\n"
          },
          {
            "name": "",
            "description": "  - {Tools[]} [init.tools] - Array of tools to enable in the instance.\n\n"
          },
          {
            "name": "",
            "description": "  - {s.HashbrownType} [init.responseSchema] - JSON schema for validating structured output.\n\n"
          },
          {
            "name": "",
            "description": "  - {Chat.Middleware[]} [init.middleware] - Middleware functions to run on messages.\n\n"
          },
          {
            "name": "",
            "description": "  - {boolean} [init.emulateStructuredOutput] - Whether to emulate structured output behavior.\n\n"
          },
          {
            "name": "",
            "description": "  - {number} [init.debounce] - Debounce interval in milliseconds for sending messages.\n\n"
          }
        ]
      }
    }
  ]
}
