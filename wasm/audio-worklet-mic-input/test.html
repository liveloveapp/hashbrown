<!doctype html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Audio Worklet Mic Input Test</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        max-width: 600px;
        margin: 50px auto;
        padding: 20px;
      }
      button {
        padding: 10px 20px;
        font-size: 16px;
        margin: 10px 5px;
        cursor: pointer;
      }
      #status {
        margin: 20px 0;
        padding: 10px;
        background: #f0f0f0;
        border-radius: 4px;
      }
      #magnitude {
        font-size: 48px;
        font-weight: bold;
        color: #333;
        margin: 20px 0;
        text-align: center;
        min-height: 60px;
      }
      #magnitudeBar {
        width: 100%;
        height: 30px;
        background: #e0e0e0;
        border-radius: 15px;
        overflow: hidden;
        margin: 10px 0;
      }
      #magnitudeBarFill {
        height: 100%;
        background: linear-gradient(to right, #4caf50, #ff9800, #f44336);
        width: 0%;
        transition: width 0.1s;
      }
    </style>
  </head>
  <body>
    <h1>Audio Worklet Microphone Input</h1>
    <button onclick="start()" id="startBtn">Start</button>
    <button onclick="stop()" id="stopBtn" disabled>Stop</button>
    <div id="status">Click Start to begin microphone input processing</div>
    <div id="magnitude">--</div>
    <div id="magnitudeBar">
      <div id="magnitudeBarFill"></div>
    </div>
    <script type="module">
      let M;
      let audioContext;
      let microphoneSource;
      let workletNode;
      let isRunning = false;

      window.logIntegerFromWorklet = (v) => {
        const magnitudeEl = document.getElementById('magnitude');
        const barFill = document.getElementById('magnitudeBarFill');
        // v is magnitude scaled by 1000, so max is around 1000 (for full scale audio)
        magnitudeEl.textContent = v;
        // Update bar (scale to percentage, cap at 100%)
        const percentage = Math.min((v / 1000) * 100, 100);
        barFill.style.width = percentage + '%';
        console.log('Magnitude:', v);
      };

      const orig = AudioWorklet.prototype.addModule;
      AudioWorklet.prototype.addModule = function (u) {
        // Fix path for worklet modules - if it doesn't already include ./output/, add it
        let fixedUrl = u;
        if (
          (u.endsWith('.aw.js') || u.endsWith('.ww.js')) &&
          !u.includes('./output/') &&
          !u.includes('/output/')
        ) {
          fixedUrl = './output/' + u;
        }
        return orig.call(this, new URL(fixedUrl, location).href);
      };

      async function getMicrophone() {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
            },
          });

          // Check the microphone's native sample rate (for information)
          const audioTrack = stream.getAudioTracks()[0];
          const settings = audioTrack.getSettings();
          if (settings.sampleRate) {
            console.log('Microphone sample rate:', settings.sampleRate, 'Hz');
          }

          return stream;
        } catch (err) {
          console.error('Error accessing microphone:', err);
          document.getElementById('status').textContent =
            'Error: Could not access microphone. ' + err.message;
          throw err;
        }
      }

      async function connectMicrophoneToWorklet() {
        if (!audioContext) {
          console.error('Audio context not initialized');
          return;
        }

        try {
          const stream = await getMicrophone();
          microphoneSource = audioContext.createMediaStreamSource(stream);

          // The processor "p" is already registered in Emscripten's AudioContext
          // Create a node using it - the processor options will be set by Emscripten
          // Actually, we can't create a new node because we don't have the callback pointer
          // Instead, let's use the worklet node that Emscripten already created
          // Or create our own node if we can get the callback

          // Get the existing worklet node that Emscripten created
          // It's stored in EmAudio using the node handle
          const nodeHandle = window._emscriptenWorkletNode;
          if (!nodeHandle) {
            throw new Error('Worklet node handle not available');
          }

          // Get the actual node object from EmAudio
          if (M.EmAudio && M.EmAudio[nodeHandle]) {
            workletNode = M.EmAudio[nodeHandle];
            console.log('Using existing Emscripten AudioWorkletNode');
          } else {
            // Fallback: try to create a new node with the callback function pointer
            // We need to get the AudioProcess function pointer from wasmTable
            // But we don't have direct access to it, so let's try a different approach
            // Actually, we can't easily get the function pointer, so let's use the existing node
            throw new Error(
              'Could not get worklet node from EmAudio. Handle: ' + nodeHandle,
            );
          }

          // Connect microphone -> worklet -> destination
          microphoneSource.connect(workletNode);
          workletNode.connect(audioContext.destination);

          document.getElementById('status').textContent =
            'Microphone connected and processing';
        } catch (err) {
          console.error('Error setting up microphone:', err);
          document.getElementById('status').textContent =
            'Error: ' + err.message;
          throw err;
        }
      }

      try {
        M = await (
          await import('./output/audio_worklet_essentials.js')
        ).default({
          locateFile: (p, pre) =>
            p.match(/\.(wasm|aw\.js|ww\.js)$/) ? './output/' + p : pre + p,
          mainScriptUrlOrBlob: new URL(
            './output/audio_worklet_essentials.js',
            location,
          ).href,
          onRuntimeInitialized: () => {
            console.log('Module ready');
            // Get the audio context from the module after initialization
            // The C code creates it, but we need to access it from JS
            // For now, we'll create our own and pass it, or get it from the module
          },
        });

        window.start = async () => {
          if (isRunning) return;
          console.log('Starting...');

          // Initialize the WASM audio worklet
          M._main();

          // Wait for worklet to be ready, then use Emscripten's AudioContext
          const checkReady = setInterval(async () => {
            try {
              // Check if worklet is ready
              if (!window.audioWorkletReady) {
                console.log('Waiting for audioWorkletReady...');
                return; // Keep waiting
              }

              console.log(
                'audioWorkletReady is true, trying to get AudioContext...',
              );

              // Try to get the AudioContext
              audioContext = window.audioWorkletNativeContext;

              // If not stored, try to get it from EmAudio using the handle
              if (!audioContext && window._emscriptenAudioContextHandle) {
                const handle = window._emscriptenAudioContextHandle;
                console.log(
                  'Trying to get AudioContext from EmAudio, handle:',
                  handle,
                );
                // Try to access EmAudio through the module
                // EmAudio might be exposed on Module now
                if (M.EmAudio && M.EmAudio[handle]) {
                  audioContext = M.EmAudio[handle];
                  console.log('Got AudioContext from M.EmAudio');
                }
              }

              // If still no context, try getting it from the worklet node
              if (!audioContext && window._emscriptenWorkletNode) {
                const nodeHandle = window._emscriptenWorkletNode;
                console.log(
                  'Trying to get AudioContext from node, handle:',
                  nodeHandle,
                );
                if (M.EmAudio && M.EmAudio[nodeHandle]) {
                  const nodeObj = M.EmAudio[nodeHandle];
                  if (nodeObj && nodeObj.context) {
                    audioContext = nodeObj.context;
                    console.log('Got AudioContext from node object');
                  }
                }
              }

              if (!audioContext) {
                console.log(
                  'AudioContext not available yet, handle:',
                  window._emscriptenAudioContextHandle,
                );
                return; // Keep waiting
              }

              console.log('Got AudioContext, resuming...');
              console.log(
                'AudioContext sample rate:',
                audioContext.sampleRate,
                'Hz',
              );

              // Resume the Emscripten audio context
              M.cwrap('ResumeAudioContext', null, [])();
              console.log('Emscripten audio context resumed');

              // Connect microphone
              await connectMicrophoneToWorklet();

              clearInterval(checkReady);
              isRunning = true;
              document.getElementById('startBtn').disabled = true;
              document.getElementById('stopBtn').disabled = false;
              document.getElementById('status').textContent =
                'Processing microphone input...';
            } catch (err) {
              console.log('Error in checkReady:', err.message);
              // Keep trying
            }
          }, 500);

          // Timeout after 10 seconds
          setTimeout(() => {
            clearInterval(checkReady);
            if (!isRunning) {
              document.getElementById('status').textContent =
                'Error: Timeout - worklet not ready. Check console for details.';
            }
          }, 10000);
        };

        window.stop = () => {
          if (!isRunning) return;

          if (microphoneSource) {
            microphoneSource.disconnect();
            microphoneSource = null;
          }
          if (workletNode) {
            workletNode.disconnect();
            workletNode = null;
          }
          if (audioContext && audioContext.state !== 'closed') {
            audioContext.close();
            audioContext = null;
          }

          isRunning = false;
          document.getElementById('startBtn').disabled = false;
          document.getElementById('stopBtn').disabled = true;
          document.getElementById('status').textContent = 'Stopped';
          document.getElementById('magnitude').textContent = '--';
          document.getElementById('magnitudeBarFill').style.width = '0%';
        };
      } catch (e) {
        console.error('Load error:', e);
        document.getElementById('status').textContent =
          'Load error: ' + e.message;
      }
    </script>
  </body>
</html>
