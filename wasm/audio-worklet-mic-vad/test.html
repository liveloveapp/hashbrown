<!doctype html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Audio Worklet Voice Activity Detection</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        max-width: 600px;
        margin: 50px auto;
        padding: 20px;
      }
      button {
        padding: 10px 20px;
        font-size: 16px;
        margin: 10px 5px;
        cursor: pointer;
      }
      #status {
        margin: 20px 0;
        padding: 10px;
        background: #f0f0f0;
        border-radius: 4px;
      }
      #vadIndicator {
        margin: 20px 0;
        padding: 40px;
        border-radius: 10px;
        font-size: 32px;
        font-weight: bold;
        text-align: center;
        transition: all 0.2s ease;
        min-height: 100px;
        display: flex;
        align-items: center;
        justify-content: center;
      }
      #vadIndicator.voice {
        background-color: #28a745;
        color: white;
      }
      #vadIndicator.no-voice {
        background-color: #6c757d;
        color: white;
      }
      #vadIndicator.idle {
        background-color: #f8f9fa;
        color: #6c757d;
        border: 2px dashed #dee2e6;
      }
    </style>
  </head>
  <body>
    <h1>Audio Worklet Voice Activity Detection</h1>
    <button onclick="start()" id="startBtn">Start</button>
    <button onclick="stop()" id="stopBtn" disabled>Stop</button>
    <div id="status">Click Start to begin voice activity detection</div>
    <div style="margin: 20px 0">
      <label for="modeSelect">VAD Mode:</label>
      <select id="modeSelect">
        <option value="0">0 - Least Aggressive (Most Sensitive)</option>
        <option value="1">1 - Low Bitrate</option>
        <option value="2" selected>2 - Aggressive</option>
        <option value="3">3 - Most Aggressive (Least Sensitive)</option>
      </select>
    </div>
    <div id="vadIndicator" class="idle">Not Recording</div>
    <script type="module">
      let M;
      let audioContext;
      let microphoneSource;
      let workletNode;
      let isRunning = false;

      window.logVADDecisionFromWorklet = (decision) => {
        const vadIndicator = document.getElementById('vadIndicator');
        // decision: 1 = voice, 0 = no voice
        if (decision === 1) {
          vadIndicator.textContent = 'ðŸŽ¤ VOICE DETECTED';
          vadIndicator.className = 'voice';
        } else {
          vadIndicator.textContent = 'No Voice';
          vadIndicator.className = 'no-voice';
        }
        console.log('VAD decision:', decision === 1 ? 'VOICE' : 'NO VOICE');
      };

      const orig = AudioWorklet.prototype.addModule;
      AudioWorklet.prototype.addModule = function (u) {
        // Fix path for worklet modules - if it doesn't already include ./output/, add it
        let fixedUrl = u;
        if (
          (u.endsWith('.aw.js') || u.endsWith('.ww.js')) &&
          !u.includes('./output/') &&
          !u.includes('/output/')
        ) {
          fixedUrl = './output/' + u;
        }
        return orig.call(this, new URL(fixedUrl, location).href);
      };

      async function getMicrophone() {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
            },
          });

          // Check the microphone's native sample rate (for information)
          const audioTrack = stream.getAudioTracks()[0];
          const settings = audioTrack.getSettings();
          if (settings.sampleRate) {
            console.log('Microphone sample rate:', settings.sampleRate, 'Hz');
          }

          return stream;
        } catch (err) {
          console.error('Error accessing microphone:', err);
          document.getElementById('status').textContent =
            'Error: Could not access microphone. ' + err.message;
          throw err;
        }
      }

      async function connectMicrophoneToWorklet() {
        if (!audioContext) {
          console.error('Audio context not initialized');
          return;
        }

        try {
          const stream = await getMicrophone();
          microphoneSource = audioContext.createMediaStreamSource(stream);

          // The processor "p" is already registered in Emscripten's AudioContext
          // Create a node using it - the processor options will be set by Emscripten
          // Actually, we can't create a new node because we don't have the callback pointer
          // Instead, let's use the worklet node that Emscripten already created
          // Or create our own node if we can get the callback

          // Get the existing worklet node that Emscripten created
          // It's stored in EmAudio using the node handle
          const nodeHandle = window._emscriptenWorkletNode;
          if (!nodeHandle) {
            throw new Error('Worklet node handle not available');
          }

          // Get the actual node object from EmAudio
          if (M.EmAudio && M.EmAudio[nodeHandle]) {
            workletNode = M.EmAudio[nodeHandle];
            console.log('Using existing Emscripten AudioWorkletNode');
          } else {
            // Fallback: try to create a new node with the callback function pointer
            // We need to get the AudioProcess function pointer from wasmTable
            // But we don't have direct access to it, so let's try a different approach
            // Actually, we can't easily get the function pointer, so let's use the existing node
            throw new Error(
              'Could not get worklet node from EmAudio. Handle: ' + nodeHandle,
            );
          }

          // Connect microphone -> worklet -> destination
          microphoneSource.connect(workletNode);
          workletNode.connect(audioContext.destination);

          document.getElementById('status').textContent =
            'Microphone connected and processing';
        } catch (err) {
          console.error('Error setting up microphone:', err);
          document.getElementById('status').textContent =
            'Error: ' + err.message;
          throw err;
        }
      }

      try {
        M = await (
          await import('./output/audio_worklet_essentials.js')
        ).default({
          locateFile: (p, pre) =>
            p.match(/\.(wasm|aw\.js|ww\.js)$/) ? './output/' + p : pre + p,
          mainScriptUrlOrBlob: new URL(
            './output/audio_worklet_essentials.js',
            location,
          ).href,
          onRuntimeInitialized: () => {
            console.log('Module ready');
            // Get the audio context from the module after initialization
            // The C code creates it, but we need to access it from JS
            // For now, we'll create our own and pass it, or get it from the module
          },
        });

        window.start = async () => {
          if (isRunning) return;
          console.log('Starting...');

          // Initialize the WASM audio worklet
          M._main();

          // Wait for worklet to be ready, then use Emscripten's AudioContext
          const checkReady = setInterval(async () => {
            try {
              // Check if worklet is ready
              if (!window.audioWorkletReady) {
                console.log('Waiting for audioWorkletReady...');
                return; // Keep waiting
              }

              // Set initial VAD mode once worklet is ready
              if (!window.vadModeInitialized) {
                const modeSelect = document.getElementById('modeSelect');
                const setVADMode = M.cwrap('SetVADMode', 'number', ['number']);
                const initialMode = parseInt(modeSelect.value);
                const result = setVADMode(initialMode);
                if (result === 0) {
                  console.log('Initial VAD mode set to:', initialMode);
                  window.vadModeInitialized = true;
                } else {
                  console.warn('Failed to set initial VAD mode:', result);
                }
              }

              console.log(
                'audioWorkletReady is true, trying to get AudioContext...',
              );

              // Try to get the AudioContext
              audioContext = window.audioWorkletNativeContext;

              // If not stored, try to get it from EmAudio using the handle
              if (!audioContext && window._emscriptenAudioContextHandle) {
                const handle = window._emscriptenAudioContextHandle;
                console.log(
                  'Trying to get AudioContext from EmAudio, handle:',
                  handle,
                );
                // Try to access EmAudio through the module
                // EmAudio might be exposed on Module now
                if (M.EmAudio && M.EmAudio[handle]) {
                  audioContext = M.EmAudio[handle];
                  console.log('Got AudioContext from M.EmAudio');
                }
              }

              // If still no context, try getting it from the worklet node
              if (!audioContext && window._emscriptenWorkletNode) {
                const nodeHandle = window._emscriptenWorkletNode;
                console.log(
                  'Trying to get AudioContext from node, handle:',
                  nodeHandle,
                );
                if (M.EmAudio && M.EmAudio[nodeHandle]) {
                  const nodeObj = M.EmAudio[nodeHandle];
                  if (nodeObj && nodeObj.context) {
                    audioContext = nodeObj.context;
                    console.log('Got AudioContext from node object');
                  }
                }
              }

              if (!audioContext) {
                console.log(
                  'AudioContext not available yet, handle:',
                  window._emscriptenAudioContextHandle,
                );
                return; // Keep waiting
              }

              console.log('Got AudioContext, resuming...');
              console.log(
                'AudioContext sample rate:',
                audioContext.sampleRate,
                'Hz',
              );

              // Resume the Emscripten audio context
              M.cwrap('ResumeAudioContext', null, [])();
              console.log('Emscripten audio context resumed');

              // Connect microphone
              await connectMicrophoneToWorklet();

              clearInterval(checkReady);
              isRunning = true;
              document.getElementById('startBtn').disabled = true;
              document.getElementById('stopBtn').disabled = false;
              document.getElementById('status').textContent =
                'Processing microphone input...';
            } catch (err) {
              console.log('Error in checkReady:', err.message);
              // Keep trying
            }
          }, 500);

          // Timeout after 10 seconds
          setTimeout(() => {
            clearInterval(checkReady);
            if (!isRunning) {
              document.getElementById('status').textContent =
                'Error: Timeout - worklet not ready. Check console for details.';
            }
          }, 10000);
        };

        window.stop = () => {
          if (!isRunning) return;

          if (microphoneSource) {
            microphoneSource.disconnect();
            microphoneSource = null;
          }
          if (workletNode) {
            workletNode.disconnect();
            workletNode = null;
          }
          if (audioContext && audioContext.state !== 'closed') {
            audioContext.close();
            audioContext = null;
          }

          isRunning = false;
          document.getElementById('startBtn').disabled = false;
          document.getElementById('stopBtn').disabled = true;
          document.getElementById('status').textContent = 'Stopped';
          const vadIndicator = document.getElementById('vadIndicator');
          vadIndicator.textContent = 'Not Recording';
          vadIndicator.className = 'idle';
        };

        // Set up VAD mode selector
        const modeSelect = document.getElementById('modeSelect');
        modeSelect.addEventListener('change', function () {
          if (M && M.cwrap) {
            try {
              const setVADMode = M.cwrap('SetVADMode', 'number', ['number']);
              const mode = parseInt(this.value);
              const result = setVADMode(mode);
              if (result === 0) {
                document.getElementById('status').textContent =
                  `VAD mode changed to ${mode}`;
                console.log('VAD mode changed to:', mode);
              } else {
                document.getElementById('status').textContent =
                  `Failed to change VAD mode (error: ${result})`;
                console.error('Failed to change VAD mode:', result);
              }
            } catch (error) {
              document.getElementById('status').textContent =
                `Error changing VAD mode: ${error.message}`;
              console.error('Mode change error:', error);
            }
          } else {
            console.warn(
              'Module not ready yet, mode change will be applied after initialization',
            );
          }
        });
      } catch (e) {
        console.error('Load error:', e);
        document.getElementById('status').textContent =
          'Load error: ' + e.message;
      }
    </script>
  </body>
</html>
